{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa2c1ada",
   "metadata": {
    "id": "aa2c1ada"
   },
   "source": [
    "# Dreambooth\n",
    "### Notebook implementation by Joe Penna (@MysteryGuitarM on Twitter) - Improvements by David Bielejeski\n",
    "\n",
    "### Instructions\n",
    "- Sign up for RunPod here: https://runpod.io/?ref=n8yfwyum\n",
    "    - Note: That's my personal referral link. Please don't use it if we are mortal enemies.\n",
    "\n",
    "- Click *Deploy* on either `SECURE CLOUD` or `COMMUNITY CLOUD`\n",
    "\n",
    "- Follow the rest of the instructions in this video: https://www.youtube.com/watch?v=7m__xadX0z0#t=5m33.1s\n",
    "\n",
    "Latest information on:\n",
    "https://github.com/JoePenna/Dreambooth-Stable-Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b971cc0",
   "metadata": {
    "id": "7b971cc0"
   },
   "source": [
    "## Build Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2AsGA1xpNQnb",
   "metadata": {
    "id": "2AsGA1xpNQnb"
   },
   "outputs": [],
   "source": [
    "# If running on Vast.AI, copy the code in this cell into a new notebook. Run it, then launch the `dreambooth_runpod_joepenna.ipynb` notebook from the jupyter interface.\n",
    "# !git clone https://github.com/JoePenna/Dreambooth-Stable-Diffusion\n",
    "!git clone https://github.com/devonbrackbill/Dreambooth-Stable-Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e1bc458-091b-42f4-a125-c3f0df20f29d",
   "metadata": {
    "id": "9e1bc458-091b-42f4-a125-c3f0df20f29d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting omegaconf\n",
      "  Downloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 4.0 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting antlr4-python3-runtime==4.9.*\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[K     |████████████████████████████████| 117 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.1.0 in /opt/conda/lib/python3.7/site-packages (from omegaconf) (6.0)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=d1465df466b1839679332dc531287dfcacde89e9e2eeca4d20d492bfa3502ac6\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/8d/53/2af8772d9aec614e3fc65e53d4a993ad73c61daa8bbd85a873\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, omegaconf\n",
      "Successfully installed antlr4-python3-runtime-4.9.3 omegaconf-2.2.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting einops\n",
      "  Downloading einops-0.5.0-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting pytorch-lightning==1.6.5\n",
      "  Downloading pytorch_lightning-1.6.5-py3-none-any.whl (585 kB)\n",
      "\u001b[K     |████████████████████████████████| 585 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchmetrics>=0.4.1\n",
      "  Downloading torchmetrics-0.10.2-py3-none-any.whl (529 kB)\n",
      "\u001b[K     |████████████████████████████████| 529 kB 8.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (4.63.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (21.3)\n",
      "Collecting protobuf<=3.20.1\n",
      "  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 9.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (4.1.1)\n",
      "Collecting tensorboard>=2.2.0\n",
      "  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (1.21.5)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (6.0)\n",
      "Collecting pyDeprecate>=0.3.1\n",
      "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: torch>=1.8.* in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==1.6.5) (1.12.0)\n",
      "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
      "  Downloading fsspec-2022.10.0-py3-none-any.whl (138 kB)\n",
      "\u001b[K     |████████████████████████████████| 138 kB 7.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.8.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (948 kB)\n",
      "\u001b[K     |████████████████████████████████| 948 kB 12.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (2.27.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (22.1.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (2.0.4)\n",
      "Collecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 7.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "\u001b[K     |████████████████████████████████| 148 kB 12.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
      "\u001b[K     |████████████████████████████████| 231 kB 11.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=17.0->pytorch-lightning==1.6.5) (3.0.9)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 4.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "\u001b[K     |████████████████████████████████| 232 kB 9.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.14.1-py2.py3-none-any.whl (175 kB)\n",
      "\u001b[K     |████████████████████████████████| 175 kB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.37.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.50.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (61.2.0)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "\u001b[K     |████████████████████████████████| 124 kB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.16.0)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 12.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (4.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (3.8.1)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 9.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (2022.6.15)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, multidict, frozenlist, cachetools, yarl, requests-oauthlib, MarkupSafe, google-auth, asynctest, async-timeout, aiosignal, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, fsspec, aiohttp, absl-py, torchmetrics, tensorboard, pyDeprecate, pytorch-lightning\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.0.1\n",
      "    Uninstalling MarkupSafe-2.0.1:\n",
      "      Successfully uninstalled MarkupSafe-2.0.1\n",
      "Successfully installed MarkupSafe-2.1.1 absl-py-1.3.0 aiohttp-3.8.3 aiosignal-1.3.1 async-timeout-4.0.2 asynctest-0.13.0 cachetools-5.2.0 frozenlist-1.3.3 fsspec-2022.10.0 google-auth-2.14.1 google-auth-oauthlib-0.4.6 grpcio-1.50.0 markdown-3.4.1 multidict-6.0.2 oauthlib-3.2.2 protobuf-3.20.1 pyDeprecate-0.3.2 pyasn1-0.4.8 pyasn1-modules-0.2.8 pytorch-lightning-1.6.5 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 torchmetrics-0.10.2 werkzeug-2.2.2 yarl-1.8.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting test-tube\n",
      "  Downloading test_tube-0.7.5.tar.gz (21 kB)\n",
      "Collecting pandas>=0.20.3\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from test-tube) (1.21.5)\n",
      "Collecting imageio>=2.3.0\n",
      "  Downloading imageio-2.22.4-py3-none-any.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorboard>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from test-tube) (2.11.0)\n",
      "Requirement already satisfied: torch>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from test-tube) (1.12.0)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.7/site-packages (from imageio>=2.3.0->test-tube) (9.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.20.3->test-tube) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.20.3->test-tube) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=0.20.3->test-tube) (1.16.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.15.0->test-tube) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.15.0->test-tube) (2.27.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.15.0->test-tube) (0.37.1)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.15.0->test-tube) (3.20.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.15.0->test-tube) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.15.0->test-tube) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.15.0->test-tube) (2.14.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.15.0->test-tube) (1.50.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.15.0->test-tube) (1.3.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.15.0->test-tube) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.15.0->test-tube) (2.2.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.15.0->test-tube) (61.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15.0->test-tube) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15.0->test-tube) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15.0->test-tube) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15.0->test-tube) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard>=1.15.0->test-tube) (4.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.15.0->test-tube) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.15.0->test-tube) (3.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.15.0->test-tube) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=1.15.0->test-tube) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=1.15.0->test-tube) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=1.15.0->test-tube) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=1.15.0->test-tube) (2.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15.0->test-tube) (3.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard>=1.15.0->test-tube) (2.1.1)\n",
      "Building wheels for collected packages: test-tube, future\n",
      "  Building wheel for test-tube (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for test-tube: filename=test_tube-0.7.5-py3-none-any.whl size=25356 sha256=7db1f92274beb4caf80f11dc038a7440fad7800b44ef7823451bf8e134aa989b\n",
      "  Stored in directory: /root/.cache/pip/wheels/1c/50/0d/15b3236957cc18a5c39ec4d4d4d21624f4d4a876756ec17064\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=43f06e5a20b5ffb30f37eee9590e7e1de0282f72d069832f85e420a4f1aa358e\n",
      "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "Successfully built test-tube future\n",
      "Installing collected packages: pandas, imageio, future, test-tube\n",
      "Successfully installed future-0.18.2 imageio-2.22.4 pandas-1.3.5 test-tube-0.7.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.5 MB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "\u001b[K     |████████████████████████████████| 163 kB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6 MB 8.2 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.12.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.6.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.10.31-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
      "\u001b[K     |████████████████████████████████| 757 kB 6.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15)\n",
      "Installing collected packages: tokenizers, regex, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.10.1 regex-2022.10.31 tokenizers-0.13.2 transformers-4.24.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting kornia\n",
      "  Downloading kornia-0.6.8-py2.py3-none-any.whl (551 kB)\n",
      "\u001b[K     |████████████████████████████████| 551 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from kornia) (21.3)\n",
      "Requirement already satisfied: torch>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from kornia) (1.12.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.8.1->kornia) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->kornia) (3.0.9)\n",
      "Installing collected packages: kornia\n",
      "Successfully installed kornia-0.6.8\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Obtaining taming-transformers from git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
      "  Cloning https://github.com/CompVis/taming-transformers.git (to revision master) to ./src/taming-transformers\n",
      "  Running command git clone -q https://github.com/CompVis/taming-transformers.git /workspace/Dreambooth-Stable-Diffusion/src/taming-transformers\n",
      "  Resolved https://github.com/CompVis/taming-transformers.git to commit 3ba01b241669f5ade541ce990f7650a3b8f65318\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from taming-transformers) (1.12.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from taming-transformers) (1.21.5)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from taming-transformers) (4.63.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->taming-transformers) (4.1.1)\n",
      "Installing collected packages: taming-transformers\n",
      "  Running setup.py develop for taming-transformers\n",
      "Successfully installed taming-transformers-0.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Obtaining clip from git+https://github.com/openai/CLIP.git@main#egg=clip\n",
      "  Cloning https://github.com/openai/CLIP.git (to revision main) to ./src/clip\n",
      "  Running command git clone -q https://github.com/openai/CLIP.git /workspace/Dreambooth-Stable-Diffusion/src/clip\n",
      "  Resolved https://github.com/openai/CLIP.git to commit d50d76daa670286dd6cacf3bcd80b5e4823fc8e1\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from clip) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from clip) (4.63.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from clip) (1.12.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from clip) (0.13.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from ftfy->clip) (0.2.5)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->clip) (4.1.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->clip) (1.21.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->clip) (2.27.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->clip) (9.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip) (2022.6.15)\n",
      "Installing collected packages: ftfy, clip\n",
      "  Running setup.py develop for clip\n",
      "Successfully installed clip-1.0 ftfy-6.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting setuptools==59.5.0\n",
      "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
      "\u001b[K     |████████████████████████████████| 952 kB 2.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 61.2.0\n",
      "    Uninstalling setuptools-61.2.0:\n",
      "      Successfully uninstalled setuptools-61.2.0\n",
      "Successfully installed setuptools-59.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: pillow==9.0.1 in /opt/conda/lib/python3.7/site-packages (9.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting torchmetrics==0.6.0\n",
      "  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
      "\u001b[K     |████████████████████████████████| 329 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.7/site-packages (from torchmetrics==0.6.0) (1.21.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from torchmetrics==0.6.0) (21.3)\n",
      "Requirement already satisfied: torch>=1.3.1 in /opt/conda/lib/python3.7/site-packages (from torchmetrics==0.6.0) (1.12.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.3.1->torchmetrics==0.6.0) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->torchmetrics==0.6.0) (3.0.9)\n",
      "Installing collected packages: torchmetrics\n",
      "  Attempting uninstall: torchmetrics\n",
      "    Found existing installation: torchmetrics 0.10.2\n",
      "    Uninstalling torchmetrics-0.10.2:\n",
      "      Successfully uninstalled torchmetrics-0.10.2\n",
      "Successfully installed torchmetrics-0.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Obtaining file:///workspace/Dreambooth-Stable-Diffusion\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from latent-diffusion==0.0.1) (1.12.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from latent-diffusion==0.0.1) (1.21.5)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from latent-diffusion==0.0.1) (4.63.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->latent-diffusion==0.0.1) (4.1.1)\n",
      "Installing collected packages: latent-diffusion\n",
      "  Running setup.py develop for latent-diffusion\n",
      "Successfully installed latent-diffusion-0.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: protobuf==3.20.1 in /opt/conda/lib/python3.7/site-packages (3.20.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting gdown\n",
      "  Downloading gdown-4.5.3.tar.gz (14 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.6.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.7/site-packages (from gdown) (2.27.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.63.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from gdown) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown) (2.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Building wheels for collected packages: gdown\n",
      "  Building wheel for gdown (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gdown: filename=gdown-4.5.3-py3-none-any.whl size=14820 sha256=f4a420a891ee9044166e798e78f4f28e0ec2ddd62a6e429d6a7bff95994efde7\n",
      "  Stored in directory: /root/.cache/pip/wheels/94/8d/0b/bdcd83555c3555f91a33f6c2384428d9f163c7d75ab0d272b4\n",
      "Successfully built gdown\n",
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-4.5.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.7/site-packages (0.10.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (2.27.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (4.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (4.1.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (4.63.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface_hub) (3.8.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (2022.6.15)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: ipywidgets==7.7.1 in /opt/conda/lib/python3.7/site-packages (7.7.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets==7.7.1) (6.15.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets==7.7.1) (0.2.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets==7.7.1) (7.31.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets==7.7.1) (5.3.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets==7.7.1) (3.6.1)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets==7.7.1) (1.1.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets==7.7.1) (0.1.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets==7.7.1) (6.2)\n",
      "Requirement already satisfied: debugpy>=1.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets==7.7.1) (1.6.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets==7.7.1) (21.3)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets==7.7.1) (7.3.4)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets==7.7.1) (5.8.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets==7.7.1) (23.2.0)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets==7.7.1) (1.5.5)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets==7.7.1) (5.1.1)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets==7.7.1) (2.11.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets==7.7.1) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets==7.7.1) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets==7.7.1) (59.5.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets==7.7.1) (3.0.20)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets==7.7.1) (4.8.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets==7.7.1) (0.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets==7.7.1) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets==7.7.1) (2.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets==7.7.1) (4.11.1)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets==7.7.1) (0.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets==7.7.1) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets==7.7.1) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets==7.7.1) (1.16.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (6.4.12)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (21.3.0)\n",
      "Requirement already satisfied: nbformat in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (5.4.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (0.14.1)\n",
      "Requirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (6.5.1)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (1.8.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (3.1.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (0.15.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (1.5.0)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (0.6.6)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (0.2.2)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (4.9.1)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (5.0.1)\n",
      "Requirement already satisfied: tinycss2 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (1.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (2.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (4.11.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (0.8.4)\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.7/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (2.16.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.7/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (4.9.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (4.12.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (22.1.0)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (4.1.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (5.9.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (0.18.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (3.8.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.7/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (2.21)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (2.3.1)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (0.5.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->ipykernel>=4.5.1->ipywidgets==7.7.1) (3.0.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting captionizer==1.0.1\n",
      "  Downloading captionizer-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: captionizer\n",
      "Successfully installed captionizer-1.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# BUILD ENV\n",
    "!pip install omegaconf\n",
    "!pip install einops\n",
    "!pip install pytorch-lightning==1.6.5\n",
    "!pip install test-tube\n",
    "!pip install transformers\n",
    "!pip install kornia\n",
    "!pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
    "!pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
    "!pip install setuptools==59.5.0\n",
    "!pip install pillow==9.0.1\n",
    "!pip install torchmetrics==0.6.0\n",
    "!pip install -e .\n",
    "!pip install protobuf==3.20.1\n",
    "!pip install gdown\n",
    "!pip install -qq diffusers[\"training\"]==0.3.0 transformers ftfy\n",
    "!pip install -qq \"ipywidgets>=7,<8\"\n",
    "!pip install huggingface_hub\n",
    "!pip install ipywidgets==7.7.1\n",
    "!pip install captionizer==1.0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dae11c10",
   "metadata": {
    "id": "dae11c10"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a3a290da2b4912a2bc4326d140c05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hugging Face Login\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f00da1ed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ model.ckpt successfully downloaded\n"
     ]
    }
   ],
   "source": [
    "# Download the 1.4 sd model\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "downloaded_model_path = hf_hub_download(\n",
    " repo_id=\"CompVis/stable-diffusion-v-1-4-original\",\n",
    " filename=\"sd-v1-4.ckpt\",\n",
    " use_auth_token=True\n",
    ")\n",
    "\n",
    "# Move the sd-v1-4.ckpt to the root of this directory as \"model.ckpt\"\n",
    "actual_locations_of_model_blob = !readlink -f {downloaded_model_path}\n",
    "!mv {actual_locations_of_model_blob[-1]} model.ckpt\n",
    "clear_output()\n",
    "print(\"✅ model.ckpt successfully downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1d11a",
   "metadata": {
    "id": "17d1d11a"
   },
   "source": [
    "# Regularization Images (Skip this section if you are uploading your own or using the provided images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed07a5df",
   "metadata": {
    "id": "ed07a5df"
   },
   "source": [
    "Training teaches your new model both your token **but** re-trains your class simultaneously.\n",
    "\n",
    "From cursory testing, it does not seem like reg images affect the model too much. However, they do affect your class greatly, which will in turn affect your generations.\n",
    "\n",
    "You can either generate your images here, or use the repos below to quickly download 1500 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f9ff0c-b529-4c7c-8e26-8388d70a5d91",
   "metadata": {
    "id": "67f9ff0c-b529-4c7c-8e26-8388d70a5d91"
   },
   "outputs": [],
   "source": [
    "# # GENERATE 200 images - Optional\n",
    "# self_generated_files_prompt = \"person\" #@param {type:\"string\"}\n",
    "# self_generated_files_count = 200 #@param {type:\"integer\"}\n",
    "\n",
    "# !python scripts/stable_txt2img.py \\\n",
    "#  --seed 10 \\\n",
    "#  --ddim_eta 0.0 \\\n",
    "#  --n_samples 1 \\\n",
    "#  --n_iter {self_generated_files_count} \\\n",
    "#  --scale 10.0 \\\n",
    "#  --ddim_steps 50 \\\n",
    "#  --ckpt model.ckpt \\\n",
    "#  --prompt {self_generated_files_prompt}\n",
    "\n",
    "# dataset=self_generated_files_prompt\n",
    "\n",
    "# !mkdir -p regularization_images/{dataset}\n",
    "# !mv outputs/txt2img-samples/*.png regularization_images/{dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c7e1c",
   "metadata": {
    "id": "3d1c7e1c"
   },
   "outputs": [],
   "source": [
    "# # Zip up the files for downloading and reuse.\n",
    "# # Download this file locally so you can reuse during another training on this dataset\n",
    "# !apt-get install -y zip\n",
    "# !zip -r regularization_images.zip regularization_images/{dataset}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad0344e",
   "metadata": {},
   "source": [
    "# Download pre-generated regularization images\n",
    "We've created the following image sets\n",
    "\n",
    "`man_euler` - provided by Niko Pueringer (Corridor Digital) - euler @ 40 steps, CFG 7.5\n",
    "`man_unsplash` - pictures from various photographers\n",
    "`person_ddim`\n",
    "`woman_ddim` - provided by David Bielejeski - ddim @ 50 steps, CFG 10.0\n",
    "`person_ddim` is recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7EydXCjOV1v",
   "metadata": {
    "id": "e7EydXCjOV1v"
   },
   "outputs": [],
   "source": [
    "# #Download Regularization Images\n",
    "\n",
    "# dataset=\"person_ddim\" #@param [\"man_euler\", \"man_unsplash\", \"person_ddim\", \"woman_ddim\", \"blonde_woman\"]\n",
    "# !git clone https://github.com/djbielejeski/Stable-Diffusion-Regularization-Images-{dataset}.git\n",
    "\n",
    "# !mkdir -p regularization_images/{dataset}\n",
    "# !mv -v Stable-Diffusion-Regularization-Images-{dataset}/{dataset}/*.* regularization_images/{dataset}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca1d28b",
   "metadata": {},
   "source": [
    "# Upload your training images\n",
    "Upload 10-20 images of someone to\n",
    "\n",
    "```\n",
    "/workspace/Dreambooth-Stable-Diffusion/training_images\n",
    "```\n",
    "\n",
    "WARNING: Be sure to upload an *even* amount of images, otherwise the training inexplicably stops at 1500 steps.\n",
    "\n",
    "*   2-3 full body\n",
    "*   3-5 upper body\n",
    "*   5-12 close-up on face\n",
    "\n",
    "The images should be:\n",
    "\n",
    "- as close as possible to the kind of images you're trying to make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02498a4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# #@markdown Add here the URLs to the images of the subject you are adding\n",
    "# urls = [\n",
    "#  \"https://i.imgur.com/test1.png\",\n",
    "#  \"https://i.imgur.com/test2.png\",\n",
    "#  \"https://i.imgur.com/test3.png\",\n",
    "#  \"https://i.imgur.com/test4.png\",\n",
    "#  \"https://i.imgur.com/test5.png\",\n",
    "#  # You can add additional images here -- about 20-30 images in different\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02250db0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# #@title Download and check the images you have just added\n",
    "# import os\n",
    "# import requests\n",
    "# from io import BytesIO\n",
    "# from PIL import Image\n",
    "\n",
    "\n",
    "# def image_grid(imgs, rows, cols):\n",
    "#  assert len(imgs) == rows*cols\n",
    "\n",
    "#  w, h = imgs[0].size\n",
    "#  grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "#  grid_w, grid_h = grid.size\n",
    "\n",
    "#  for i, img in enumerate(imgs):\n",
    "#   grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "#  return grid\n",
    "\n",
    "# def download_image(url):\n",
    "#  try:\n",
    "#   response = requests.get(url)\n",
    "#  except:\n",
    "#   return None\n",
    "#  return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "# images = list(filter(None,[download_image(url) for url in urls]))\n",
    "# save_path = \"./training_images\"\n",
    "# if not os.path.exists(save_path):\n",
    "#  os.mkdir(save_path)\n",
    "# [image.save(f\"{save_path}/{i}.png\", format=\"png\") for i, image in enumerate(images)]\n",
    "# image_grid(images, 1, len(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472b480f-9ab7-44a2-a23c-d056500cabcb",
   "metadata": {},
   "source": [
    "# Rename all the icons\n",
    "\n",
    "Upload icons to `workplace/Dreambooth-Stable-Diffusion/res/regularization_images`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e61ac32-003f-4931-9806-1960a9a04165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 images in token_class\n",
      "directory already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "files = os.listdir('res/regularization_images/')\n",
    "\n",
    "# pick 40 random images\n",
    "random.seed(1234)\n",
    "files = [file for file in files if 'person' in file] # filter for all images with 'person' in the title\n",
    "#files = random.choices(files, k=40)\n",
    "#assert(len(files) == 40)\n",
    "print('{} images in token_class'.format(len(files)))\n",
    "\n",
    "# make a directory for the training (token_class)\n",
    "try:\n",
    "    os.mkdir('res/token_class')\n",
    "except:\n",
    "    print('directory already exists')\n",
    "\n",
    "# and move them to the training directory\n",
    "counter = 0\n",
    "for file in files:\n",
    "    os.rename(os.path.join('res/regularization_images', file),\n",
    "              os.path.join('res/token_class', 'personicontest' + str(counter).zfill(3) + '.png'))\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb19db-4d38-4784-9fbe-b559af36359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_dir = 'res/fontawesome-png/'\n",
    "# img_dir_out = 'res/fontawesome-png-ready/'\n",
    "# import os\n",
    "# os.mkdir(img_dir_out)\n",
    "# files = os.listdir(img_dir)\n",
    "# # rename\n",
    "# counter = 0\n",
    "# for file in files:\n",
    "#     os.rename(os.path.join(img_dir, file),\n",
    "#               os.path.join(img_dir_out, 'randomicontest' + str(counter).zfill(3) + '.png'))\n",
    "#     counter += 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b32e1-06c5-446e-b481-44502047ea24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad4e50df",
   "metadata": {
    "id": "ad4e50df"
   },
   "source": [
    "## Training\n",
    "\n",
    "If training a person or subject, keep an eye on your project's `logs/{folder}/images/train/samples_scaled_gs-00xxxx` generations.\n",
    "\n",
    "If training a style, keep an eye on your project's `logs/{folder}/images/train/samples_gs-00xxxx` generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fa5dd66-2ca0-4819-907e-802e25583ae6",
   "metadata": {
    "id": "6fa5dd66-2ca0-4819-907e-802e25583ae6",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "Moving 0 files to the new cache system\n",
      "0it [00:00, ?it/s]\n",
      "Global seed set to 23\n",
      "Running on GPUs 0,\n",
      "Loading model from model.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 64, 64) = 16384 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Downloading: 100%|███████████████████████████| 961k/961k [00:00<00:00, 6.73MB/s]\n",
      "Downloading: 100%|███████████████████████████| 525k/525k [00:00<00:00, 5.30MB/s]\n",
      "Downloading: 100%|██████████████████████████████| 389/389 [00:00<00:00, 393kB/s]\n",
      "Downloading: 100%|██████████████████████████████| 905/905 [00:00<00:00, 907kB/s]\n",
      "Downloading: 100%|█████████████████████████| 4.52k/4.52k [00:00<00:00, 3.21MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.71G/1.71G [05:55<00:00, 4.82MB/s]\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'logit_scale', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'visual_projection.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Restored from model.ckpt with 0 missing and 2 unexpected keys\n",
      "Unexpected Keys: ['model_ema.decay', 'model_ema.num_updates']\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loggers/test_tube.py:106: LightningDeprecationWarning: The TestTubeLogger is deprecated since v1.5 and will be removed in v1.7. We recommend switching to the `pytorch_lightning.loggers.TensorBoardLogger` as an alternative.\n",
      "  \"The TestTubeLogger is deprecated since v1.5 and will be removed in v1.7. We recommend switching to the\"\n",
      "Monitoring val/loss_simple_ema as checkpoint metric.\n",
      "Merged modelckpt-cfg: \n",
      "{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/token_class2022-11-09T12-43-09_personicontest/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 1, 'every_n_train_steps': 500}}\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "#### Data #####\n",
      "train, PersonalizedBase, 4800\n",
      "reg, PersonalizedBase, 13390\n",
      "validation, PersonalizedBase, 48\n",
      "accumulate_grad_batches = 1\n",
      "++++ NOT USING LR SCALING ++++\n",
      "Setting learning rate to 1.00e-06\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:327: LightningDeprecationWarning: Base `LightningModule.on_train_batch_start` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "  f\"Base `LightningModule.{hook}` hook signature has changed in v1.5.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:336: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "  \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:392: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "  f\"The `Callback.{hook}` hook has been deprecated in v1.6 and\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:343: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "  f\"Base `Callback.{hook}` hook signature has changed in v1.5.\"\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LatentDiffusion: Also optimizing conditioner params!\n",
      "\n",
      "  | Name              | Type               | Params\n",
      "---------------------------------------------------------\n",
      "0 | model             | DiffusionWrapper   | 859 M \n",
      "1 | first_stage_model | AutoencoderKL      | 83.7 M\n",
      "2 | cond_stage_model  | FrozenCLIPEmbedder | 123 M \n",
      "---------------------------------------------------------\n",
      "982 M     Trainable params\n",
      "83.7 M    Non-trainable params\n",
      "1.1 B     Total params\n",
      "4,264.941 Total estimated model params size (MB)\n",
      "Project config\n",
      "model:\n",
      "  base_learning_rate: 1.0e-06\n",
      "  target: ldm.models.diffusion.ddpm.LatentDiffusion\n",
      "  params:\n",
      "    reg_weight: 1.0\n",
      "    linear_start: 0.00085\n",
      "    linear_end: 0.012\n",
      "    num_timesteps_cond: 1\n",
      "    log_every_t: 200\n",
      "    timesteps: 1000\n",
      "    first_stage_key: image\n",
      "    cond_stage_key: caption\n",
      "    image_size: 64\n",
      "    channels: 4\n",
      "    cond_stage_trainable: true\n",
      "    conditioning_key: crossattn\n",
      "    monitor: val/loss_simple_ema\n",
      "    scale_factor: 0.18215\n",
      "    use_ema: false\n",
      "    embedding_reg_weight: 0.0\n",
      "    unfreeze_model: true\n",
      "    model_lr: 1.0e-06\n",
      "    personalization_config:\n",
      "      target: ldm.modules.embedding_manager.EmbeddingManager\n",
      "      params:\n",
      "        placeholder_strings:\n",
      "        - '*'\n",
      "        initializer_words:\n",
      "        - sculpture\n",
      "        per_image_tokens: false\n",
      "        num_vectors_per_token: 1\n",
      "        progressive_words: false\n",
      "    unet_config:\n",
      "      target: ldm.modules.diffusionmodules.openaimodel.UNetModel\n",
      "      params:\n",
      "        image_size: 32\n",
      "        in_channels: 4\n",
      "        out_channels: 4\n",
      "        model_channels: 320\n",
      "        attention_resolutions:\n",
      "        - 4\n",
      "        - 2\n",
      "        - 1\n",
      "        num_res_blocks: 2\n",
      "        channel_mult:\n",
      "        - 1\n",
      "        - 2\n",
      "        - 4\n",
      "        - 4\n",
      "        num_heads: 8\n",
      "        use_spatial_transformer: true\n",
      "        transformer_depth: 1\n",
      "        context_dim: 768\n",
      "        use_checkpoint: true\n",
      "        legacy: false\n",
      "    first_stage_config:\n",
      "      target: ldm.models.autoencoder.AutoencoderKL\n",
      "      params:\n",
      "        embed_dim: 4\n",
      "        monitor: val/rec_loss\n",
      "        ddconfig:\n",
      "          double_z: true\n",
      "          z_channels: 4\n",
      "          resolution: 512\n",
      "          in_channels: 3\n",
      "          out_ch: 3\n",
      "          ch: 128\n",
      "          ch_mult:\n",
      "          - 1\n",
      "          - 2\n",
      "          - 4\n",
      "          - 4\n",
      "          num_res_blocks: 2\n",
      "          attn_resolutions: []\n",
      "          dropout: 0.0\n",
      "        lossconfig:\n",
      "          target: torch.nn.Identity\n",
      "    cond_stage_config:\n",
      "      target: ldm.modules.encoders.modules.FrozenCLIPEmbedder\n",
      "    ckpt_path: model.ckpt\n",
      "data:\n",
      "  target: main.DataModuleFromConfig\n",
      "  params:\n",
      "    batch_size: 1\n",
      "    num_workers: 1\n",
      "    wrap: false\n",
      "    train:\n",
      "      target: ldm.data.personalized.PersonalizedBase\n",
      "      params:\n",
      "        size: 512\n",
      "        set: train\n",
      "        per_image_tokens: false\n",
      "        repeats: 100\n",
      "        coarse_class_text: black and white icon\n",
      "        data_root: /workspace/Dreambooth-Stable-Diffusion/res/token_class\n",
      "        placeholder_token: personicontest\n",
      "        token_only: false\n",
      "    reg:\n",
      "      target: ldm.data.personalized.PersonalizedBase\n",
      "      params:\n",
      "        size: 512\n",
      "        set: train\n",
      "        reg: true\n",
      "        per_image_tokens: false\n",
      "        repeats: 10\n",
      "        data_root: /workspace/Dreambooth-Stable-Diffusion/res/regularization_images/\n",
      "        coarse_class_text: black and white icon\n",
      "        placeholder_token: personicontest\n",
      "    validation:\n",
      "      target: ldm.data.personalized.PersonalizedBase\n",
      "      params:\n",
      "        size: 512\n",
      "        set: val\n",
      "        per_image_tokens: false\n",
      "        repeats: 10\n",
      "        coarse_class_text: black and white icon\n",
      "        placeholder_token: personicontest\n",
      "        data_root: /workspace/Dreambooth-Stable-Diffusion/res/token_class\n",
      "\n",
      "Lightning config\n",
      "modelcheckpoint:\n",
      "  params:\n",
      "    every_n_train_steps: 500\n",
      "callbacks:\n",
      "  image_logger:\n",
      "    target: main.ImageLogger\n",
      "    params:\n",
      "      batch_frequency: 500\n",
      "      max_images: 8\n",
      "      increase_log_steps: false\n",
      "trainer:\n",
      "  benchmark: true\n",
      "  max_steps: 2000\n",
      "  gpus: 0,\n",
      "\n",
      "Sanity Checking: 0it [00:00, ?it/s]/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n",
      "Training: 0it [00:00, ?it/s]/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:2103: LightningDeprecationWarning: `Trainer.root_gpu` is deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.strategy.root_device.index` instead.\n",
      "  \"`Trainer.root_gpu` is deprecated in v1.6 and will be removed in v1.8. \"\n",
      "Epoch 0:   0%|                                         | 0/4848 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/data.py:73: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:231: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
      "Epoch 0:  10%| | 499/4848 [11:53<1:43:42,  1.43s/it, loss=0.0162, v_num=0, trainData shape for DDIM sampling is (1, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:06,  7.92it/s]\u001b[A\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:00<00:05,  8.06it/s]\u001b[A\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:00<00:05,  8.10it/s]\u001b[A\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:00<00:05,  8.13it/s]\u001b[A\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:00<00:05,  8.13it/s]\u001b[A\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:00<00:05,  8.13it/s]\u001b[A\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:00<00:05,  8.14it/s]\u001b[A\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:00<00:05,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:01<00:05,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:01<00:04,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:01<00:04,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:01<00:04,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:01<00:04,  8.14it/s]\u001b[A\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:01<00:04,  8.13it/s]\u001b[A\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:01<00:04,  8.13it/s]\u001b[A\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:01<00:04,  8.14it/s]\u001b[A\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:02<00:04,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:02<00:03,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:02<00:03,  8.14it/s]\u001b[A\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:02<00:03,  8.13it/s]\u001b[A\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:02<00:03,  8.13it/s]\u001b[A\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:02<00:03,  8.14it/s]\u001b[A\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:02<00:03,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:02<00:03,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:03<00:03,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:03<00:02,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:03<00:02,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:03<00:02,  8.14it/s]\u001b[A\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:03<00:02,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:03<00:02,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:03<00:02,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:03<00:02,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:04<00:02,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:04<00:01,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:04<00:01,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:04<00:01,  8.14it/s]\u001b[A\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:04<00:01,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:04<00:01,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:04<00:01,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:04<00:01,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:05<00:01,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:05<00:00,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:05<00:00,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:05<00:00,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:05<00:00,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:05<00:00,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:05<00:00,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:05<00:00,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:06<00:00,  8.16it/s]\u001b[A\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:06<00:00,  8.15it/s]\u001b[A\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:07,  1.37s/it]\u001b[A\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:32,  1.49it/s]\u001b[A\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:01<00:20,  2.24it/s]\u001b[A\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:01<00:15,  2.93it/s]\u001b[A\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:02<00:12,  3.53it/s]\u001b[A\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:02<00:10,  4.02it/s]\u001b[A\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:02<00:09,  4.42it/s]\u001b[A\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:02<00:08,  4.72it/s]\u001b[A\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:02<00:08,  4.95it/s]\u001b[A\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:02<00:07,  5.12it/s]\u001b[A\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:03<00:07,  5.24it/s]\u001b[A\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:03<00:07,  5.32it/s]\u001b[A\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:03<00:06,  5.38it/s]\u001b[A\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:03<00:06,  5.42it/s]\u001b[A\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:03<00:06,  5.45it/s]\u001b[A\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:04<00:06,  5.48it/s]\u001b[A\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:04<00:06,  5.49it/s]\u001b[A\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:04<00:05,  5.50it/s]\u001b[A\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:04<00:05,  5.51it/s]\u001b[A\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:04<00:05,  5.51it/s]\u001b[A\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:04<00:05,  5.52it/s]\u001b[A\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:05<00:05,  5.52it/s]\u001b[A\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:05<00:04,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:05<00:04,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:05<00:04,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:05<00:04,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:06<00:04,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:06<00:03,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:06<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:06<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:06<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:06<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:07<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:07<00:02,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:07<00:02,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:07<00:02,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:07<00:02,  5.52it/s]\u001b[A\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:08<00:02,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:08<00:01,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:08<00:01,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:08<00:01,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:08<00:01,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:08<00:01,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:09<00:01,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:09<00:00,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:09<00:00,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:09<00:00,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:09<00:00,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:10<00:00,  5.53it/s]\u001b[A\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:10<00:00,  4.89it/s]\u001b[A\n",
      "Epoch 0:  10%| | 500/4848 [12:13<1:46:16,  1.47s/it, loss=0.0162, v_num=0, train/opt/conda/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:378: UserWarning: `ModelCheckpoint(monitor='val/loss_simple_ema')` could not find the monitored key in the returned metrics: ['train/loss_simple', 'train/loss_simple_step', 'train/loss_vlb', 'train/loss_vlb_step', 'train/loss', 'train/loss_step', 'global_step', 'epoch', 'step']. HINT: Did you call `log('val/loss_simple_ema', value)` in the `LightningModule`?\n",
      "  warning_cache.warn(m)\n",
      "Epoch 0, global step 500: 'val/loss_simple_ema' was not in top 1\n",
      "Prunin' Checkpoint\n",
      "Checkpoint Keys: dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n",
      "Removing optimizer states from checkpoint\n",
      "This is global step 500.\n",
      "Epoch 0:  21%|▏| 999/4848 [24:09<1:33:06,  1.45s/it, loss=0.00987, v_num=0, traipop from empty list\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:06,  7.90it/s]\u001b[A\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:00<00:05,  8.05it/s]\u001b[A\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:00<00:05,  8.10it/s]\u001b[A\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:00<00:05,  8.13it/s]\u001b[A\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:00<00:05,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:00<00:05,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:00<00:05,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:00<00:05,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:01<00:05,  8.10it/s]\u001b[A\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:01<00:04,  8.12it/s]\u001b[A\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:01<00:04,  8.12it/s]\u001b[A\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:01<00:04,  8.14it/s]\u001b[A\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:01<00:04,  8.11it/s]\u001b[A\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:01<00:04,  8.13it/s]\u001b[A\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:01<00:04,  8.11it/s]\u001b[A\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:01<00:04,  8.12it/s]\u001b[A\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:02<00:04,  8.14it/s]\u001b[A\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:02<00:03,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:02<00:03,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:02<00:03,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:02<00:03,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:02<00:03,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:02<00:03,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:02<00:03,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:03<00:03,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:03<00:02,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:03<00:02,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:03<00:02,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:03<00:02,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:03<00:02,  8.13it/s]\u001b[A\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:03<00:02,  8.14it/s]\u001b[A\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:03<00:02,  8.13it/s]\u001b[A\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:04<00:02,  8.14it/s]\u001b[A\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:04<00:01,  8.12it/s]\u001b[A\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:04<00:01,  8.14it/s]\u001b[A\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:04<00:01,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:04<00:01,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:04<00:01,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:04<00:01,  8.14it/s]\u001b[A\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:04<00:01,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:05<00:01,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:05<00:00,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:05<00:00,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:05<00:00,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:05<00:00,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:05<00:00,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:05<00:00,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:05<00:00,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:06<00:00,  8.12it/s]\u001b[A\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:06<00:00,  8.15it/s]\u001b[A\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:08,  5.51it/s]\u001b[A\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:00<00:08,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:00<00:08,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:00<00:08,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:00<00:08,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:01<00:07,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:01<00:07,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:01<00:07,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:01<00:07,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:01<00:07,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:01<00:07,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:02<00:06,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:02<00:06,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:02<00:06,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:02<00:06,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:02<00:06,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:03<00:05,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:03<00:05,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:03<00:05,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:03<00:05,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:03<00:05,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:03<00:05,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:04<00:04,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:04<00:04,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:04<00:04,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:04<00:04,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:04<00:04,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:05<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:05<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:05<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:05<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:05<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:05<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:06<00:02,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:06<00:02,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:06<00:02,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:06<00:02,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:06<00:02,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:07<00:01,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:07<00:01,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:07<00:01,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:07<00:01,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:07<00:01,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:07<00:01,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:08<00:00,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:08<00:00,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:08<00:00,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:08<00:00,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:08<00:00,  5.53it/s]\u001b[A\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:09<00:00,  5.54it/s]\u001b[A\n",
      "Epoch 0:  21%|▏| 1000/4848 [24:27<1:34:05,  1.47s/it, loss=0.0083, v_num=0, traiEpoch 0, global step 1000: 'val/loss_simple_ema' was not in top 1\n",
      "Prunin' Checkpoint\n",
      "Checkpoint Keys: dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n",
      "Removing optimizer states from checkpoint\n",
      "This is global step 1000.\n",
      "Epoch 0:  31%|▎| 1499/4848 [36:24<1:21:20,  1.46s/it, loss=0.0102, v_num=0, traipop from empty list\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:06,  7.90it/s]\u001b[A\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:00<00:05,  8.05it/s]\u001b[A\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:00<00:05,  8.10it/s]\u001b[A\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:00<00:05,  8.13it/s]\u001b[A\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:00<00:05,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:00<00:05,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:00<00:05,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:00<00:05,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:01<00:05,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:01<00:04,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:01<00:04,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:01<00:04,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:01<00:04,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:01<00:04,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:01<00:04,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:01<00:04,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:02<00:04,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:02<00:03,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:02<00:03,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:02<00:03,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:02<00:03,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:02<00:03,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:02<00:03,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:02<00:03,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:03<00:03,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:03<00:02,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:03<00:02,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:03<00:02,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:03<00:02,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:03<00:02,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:03<00:02,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:03<00:02,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:04<00:02,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:04<00:01,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:04<00:01,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:04<00:01,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:04<00:01,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:04<00:01,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:04<00:01,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:04<00:01,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:05<00:01,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:05<00:00,  8.13it/s]\u001b[A\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:05<00:00,  8.14it/s]\u001b[A\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:05<00:00,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:05<00:00,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:05<00:00,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:05<00:00,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:05<00:00,  8.19it/s]\u001b[A\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:05<00:00,  8.18it/s]\u001b[A\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:06<00:00,  8.17it/s]\u001b[A\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:08,  5.51it/s]\u001b[A\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:00<00:08,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:00<00:08,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:00<00:08,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:00<00:08,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:01<00:07,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:01<00:07,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:01<00:07,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:01<00:07,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:01<00:07,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:01<00:07,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:02<00:06,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:02<00:06,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:02<00:06,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:02<00:06,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:02<00:06,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:03<00:05,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:03<00:05,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:03<00:05,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:03<00:05,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:03<00:05,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:03<00:05,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:04<00:04,  5.55it/s]\u001b[A\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:04<00:04,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:04<00:04,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:04<00:04,  5.55it/s]\u001b[A\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:04<00:04,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:05<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:05<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:05<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:05<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:05<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:05<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:06<00:02,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:06<00:02,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:06<00:02,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:06<00:02,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:06<00:02,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:07<00:01,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:07<00:01,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:07<00:01,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:07<00:01,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:07<00:01,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:07<00:01,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:08<00:00,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:08<00:00,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:08<00:00,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:08<00:00,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:08<00:00,  5.53it/s]\u001b[A\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:09<00:00,  5.54it/s]\u001b[A\n",
      "Epoch 0:  31%|▎| 1500/4848 [36:41<1:21:54,  1.47s/it, loss=0.0101, v_num=0, traiEpoch 0, global step 1500: 'val/loss_simple_ema' was not in top 1\n",
      "Prunin' Checkpoint\n",
      "Checkpoint Keys: dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n",
      "Removing optimizer states from checkpoint\n",
      "This is global step 1500.\n",
      "Epoch 0:  41%|▍| 1999/4848 [48:40<1:09:21,  1.46s/it, loss=0.0107, v_num=0, traipop from empty list\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:06,  7.93it/s]\u001b[A\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:00<00:05,  8.07it/s]\u001b[A\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:00<00:05,  8.12it/s]\u001b[A\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:00<00:05,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:00<00:05,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:00<00:05,  8.13it/s]\u001b[A\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:00<00:05,  8.08it/s]\u001b[A\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:00<00:05,  8.12it/s]\u001b[A\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:01<00:05,  8.10it/s]\u001b[A\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:01<00:04,  8.13it/s]\u001b[A\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:01<00:04,  8.11it/s]\u001b[A\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:01<00:04,  8.12it/s]\u001b[A\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:01<00:04,  8.12it/s]\u001b[A\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:01<00:04,  8.13it/s]\u001b[A\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:01<00:04,  8.13it/s]\u001b[A\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:01<00:04,  8.14it/s]\u001b[A\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:02<00:04,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:02<00:03,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:02<00:03,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:02<00:03,  8.14it/s]\u001b[A\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:02<00:03,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:02<00:03,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:02<00:03,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:02<00:03,  8.15it/s]\u001b[A\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:03<00:03,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:03<00:02,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:03<00:02,  8.16it/s]\u001b[A\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:03<00:02,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:03<00:02,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:03<00:02,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:03<00:02,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:03<00:02,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:04<00:02,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:04<00:01,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:04<00:01,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:04<00:01,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:04<00:01,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:04<00:01,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:04<00:01,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:04<00:01,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:05<00:01,  8.19it/s]\u001b[A\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:05<00:00,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:05<00:00,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:05<00:00,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:05<00:00,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:05<00:00,  8.17it/s]\u001b[A\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:05<00:00,  8.18it/s]\u001b[A\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:05<00:00,  8.19it/s]\u001b[A\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:06<00:00,  8.19it/s]\u001b[A\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:06<00:00,  8.16it/s]\u001b[A\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:08,  5.51it/s]\u001b[A\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:00<00:08,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:00<00:08,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:00<00:08,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:00<00:08,  5.55it/s]\u001b[A\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:01<00:07,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:01<00:07,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:01<00:07,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:01<00:07,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:01<00:07,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:01<00:07,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:02<00:06,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:02<00:06,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:02<00:06,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:02<00:06,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:02<00:06,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:03<00:05,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:03<00:05,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:03<00:05,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:03<00:05,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:03<00:05,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:03<00:05,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:04<00:04,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:04<00:04,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:04<00:04,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:04<00:04,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:04<00:04,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:05<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:05<00:03,  5.55it/s]\u001b[A\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:05<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:05<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:05<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:05<00:03,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:06<00:02,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:06<00:02,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:06<00:02,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:06<00:02,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:06<00:02,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:07<00:01,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:07<00:01,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:07<00:01,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:07<00:01,  5.55it/s]\u001b[A\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:07<00:01,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:07<00:01,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:08<00:00,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:08<00:00,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:08<00:00,  5.54it/s]\u001b[A\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:08<00:00,  5.53it/s]\u001b[A\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:08<00:00,  5.53it/s]\u001b[A\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:09<00:00,  5.54it/s]\u001b[A\n",
      "Epoch 0:  41%|▍| 2000/4848 [48:57<1:09:42,  1.47s/it, loss=0.0107, v_num=0, traiEpoch 0, global step 2000: 'val/loss_simple_ema' was not in top 1\n",
      "Prunin' Checkpoint\n",
      "Checkpoint Keys: dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n",
      "Removing optimizer states from checkpoint\n",
      "This is global step 2000.\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:2029: LightningDeprecationWarning: `Trainer.training_type_plugin` is deprecated in v1.6 and will be removed in v1.8. Use `Trainer.strategy` instead.\n",
      "  \"`Trainer.training_type_plugin` is deprecated in v1.6 and will be removed in v1.8. Use\"\n",
      "Average Epoch time: 2942.18 seconds\n",
      "Average Peak memory 20613.30MiB\n",
      "Epoch 0:  41%|▍| 2000/4848 [49:02<1:09:49,  1.47s/it, loss=0.0107, v_num=0, trai\n",
      "Training complete. max_training_steps reached or we blew up.\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "# This isn't used for training, just to help you remember what your trained into the model.\n",
    "project_name = \"personicontest\"\n",
    "\n",
    "# MAX STEPS\n",
    "# How many steps do you want to train for?\n",
    "max_training_steps = 2000\n",
    "\n",
    "# Match class_word to the category of the regularization images you chose above.\n",
    "class_word = \"black and white icon\" # typical uses are \"man\", \"person\", \"woman\"\n",
    "\n",
    "# This is the unique token you are incorporating into the stable diffusion model.\n",
    "token = \"personicontest\"\n",
    "\n",
    "reg_data_root = \"/workspace/Dreambooth-Stable-Diffusion/res/regularization_images/\"\n",
    "\n",
    "!rm -rf training_images/.ipynb_checkpoints\n",
    "!python \"main.py\" \\\n",
    " --base configs/stable-diffusion/v1-finetune_unfrozen.yaml \\\n",
    " -t \\\n",
    " --actual_resume \"model.ckpt\" \\\n",
    " --reg_data_root \"{reg_data_root}\" \\\n",
    " -n \"{project_name}\" \\\n",
    " --gpus 0, \\\n",
    " --data_root \"/workspace/Dreambooth-Stable-Diffusion/res/token_class\" \\\n",
    " --max_training_steps {max_training_steps} \\\n",
    " --class_word \"{class_word}\" \\\n",
    " --token \"{token}\" \\\n",
    " --no-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc49d0bd",
   "metadata": {},
   "source": [
    "## Copy and name the checkpoint file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff5a5f3a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download your trained model file from trained_models/2022-11-08T02-06-00_randomicontest_1_training_images_1500_max_training_steps_randomicontest_token_black_SVG_icon_class_word.ckpt and use in your favorite Stable Diffusion repo!\n"
     ]
    }
   ],
   "source": [
    "# # Copy the checkpoint into our `trained_models` folder\n",
    "\n",
    "# directory_paths = !ls -d logs/*\n",
    "# last_checkpoint_file = directory_paths[-1] + \"/checkpoints/last.ckpt\"\n",
    "# training_images = !find training_images/*\n",
    "# date_string = !date +\"%Y-%m-%dT%H-%M-%S\"\n",
    "# file_name = date_string[-1] + \"_\" + project_name + \"_\" + str(len(training_images)) + \"_training_images_\" +  str(max_training_steps) + \"_max_training_steps_\" + token + \"_token_\" + class_word + \"_class_word.ckpt\"\n",
    "\n",
    "# file_name = file_name.replace(\" \", \"_\")\n",
    "\n",
    "# !mkdir -p trained_models\n",
    "# !mv \"{last_checkpoint_file}\" \"trained_models/{file_name}\"\n",
    "\n",
    "# print(\"Download your trained model file from trained_models/\" + file_name + \" and use in your favorite Stable Diffusion repo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aad34f",
   "metadata": {},
   "source": [
    "# Optional - Upload to google drive\n",
    "* run the following commands in a new `terminal` in the `Dreambooth-Stable-Diffusion` directory\n",
    "* `chmod +x ./gdrive`\n",
    "* `./gdrive about`\n",
    "* `paste your token here after navigating to the link`\n",
    "* `./gdrive upload trained_models/{file_name.ckpt}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90ac5c",
   "metadata": {},
   "source": [
    "# Big Important Note!\n",
    "\n",
    "The way to use your token is `<token> <class>` ie `joepenna person` and not just `joepenna`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28d0139",
   "metadata": {},
   "source": [
    "## Generate Images With Your Trained Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29a7afd8-1097-4d1b-a274-6c904378e501",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'logit_scale', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'visual_projection.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'text_projection.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:33,  1.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<00:59,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:47,  1.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:41,  1.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:04<00:38,  1.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:07<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:30,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:29,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:28,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:26,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:25,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:24,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:14<00:24,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:22,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:21,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:19,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:18,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:15,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:24<00:14,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:12,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:09,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:31<00:07,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:40<00:00, 40.63s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:40<00:40, 40.63s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:36,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:35,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:33,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:32,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:29,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:09<00:28,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:16<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:19<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:23<00:14,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:26<00:11,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:29<00:08,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:33<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:36<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.47s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:20<00:00, 40.05s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'logit_scale', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'visual_projection.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'text_projection.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:34,  1.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<00:59,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:04<00:38,  1.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:14<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:24<00:14,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:31<00:07,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.18s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.18s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:23<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.67s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:20<00:00, 40.42s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'text_projection.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'logit_scale', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:34,  1.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<00:59,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:04<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:14<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:31<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.25s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.25s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:23<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.71s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:20<00:00, 40.48s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'logit_scale', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:33,  1.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<00:59,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:04<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:14<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:31<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.27s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.27s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:24<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.73s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:21<00:00, 40.50s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'logit_scale', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:34,  1.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<00:59,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:04<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:31<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.27s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.27s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:24<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.74s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:21<00:00, 40.50s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'logit_scale', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'text_projection.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:33,  1.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<00:59,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:04<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:14<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:31<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.25s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.25s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:23<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.71s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:20<00:00, 40.48s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'logit_scale', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'text_projection.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.post_layernorm.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:34,  1.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<00:59,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:04<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:31<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.25s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.25s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:24<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.71s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:20<00:00, 40.48s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'logit_scale', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'text_projection.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:34,  1.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<00:59,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:04<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:31<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.27s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.27s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:24<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.72s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:20<00:00, 40.50s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'logit_scale', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:34,  1.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<00:59,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:04<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:31<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.28s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.28s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:23<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.70s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:20<00:00, 40.49s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'logit_scale', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'visual_projection.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:34,  1.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<00:59,  1.25s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:04<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:32<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.28s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.28s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:24<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.73s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:21<00:00, 40.50s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'text_projection.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'visual_projection.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'logit_scale', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:33,  1.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<00:59,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:04<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:31<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.26s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.26s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:24<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.73s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:20<00:00, 40.49s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'logit_scale', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:34,  1.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<00:59,  1.25s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:05<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:32<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.27s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.27s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:24<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.72s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:20<00:00, 40.50s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'text_projection.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'logit_scale', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:34,  1.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<00:59,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:04<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:31<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.26s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.26s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:24<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.73s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:20<00:00, 40.50s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'logit_scale', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:34,  1.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<00:59,  1.25s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:05<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:32<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.28s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.28s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:24<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.73s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:21<00:00, 40.50s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'text_projection.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'logit_scale', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:33,  1.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<00:59,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:04<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:31<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.27s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.27s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:24<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.73s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:21<00:00, 40.50s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'logit_scale', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'visual_projection.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:34,  1.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<00:59,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:04<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:32<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.27s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.27s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:24<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.72s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:20<00:00, 40.50s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'logit_scale', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'visual_projection.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:34,  1.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<00:59,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:04<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:14<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:31<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.27s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.27s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:24<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.72s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:20<00:00, 40.49s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'visual_projection.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'logit_scale', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:34,  1.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<00:59,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:04<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:14<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:31<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.24s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.24s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:23<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.71s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:20<00:00, 40.48s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'logit_scale', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:35,  1.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<01:00,  1.25s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:05<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:31<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.27s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.27s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:23<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.70s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:20<00:00, 40.48s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'visual_projection.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'logit_scale', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:37,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<01:00,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:05<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:32<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.30s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.30s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:23<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.69s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:20<00:00, 40.49s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'logit_scale', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:37,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<01:00,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:05<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:35,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:32<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.30s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.30s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:23<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.67s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:20<00:00, 40.48s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'text_projection.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'logit_scale', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'visual_projection.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:37,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<01:00,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:05<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:32<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.30s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.30s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:23<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.71s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:21<00:00, 40.51s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'text_projection.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'logit_scale', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:37,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<01:00,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:05<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:34,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:32<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.29s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.29s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:23<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.70s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:20<00:00, 40.49s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'text_projection.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'logit_scale', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm2.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:37,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<01:00,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:05<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:35,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:21<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:32<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.35s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.35s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:23<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.71s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:21<00:00, 40.53s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'visual_projection.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'logit_scale', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm2.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:37,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<01:01,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:49,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:05<00:39,  1.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:35,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:22<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:32<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:39<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.37s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.37s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:24<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.73s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:21<00:00, 40.55s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'logit_scale', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:37,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<01:01,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:05<00:39,  1.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:35,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:22<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:32<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:39<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.35s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.35s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:24<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.70s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:21<00:00, 40.52s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'logit_scale', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.embeddings.position_ids', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'text_projection.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:37,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<01:00,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:05<00:39,  1.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:35,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:22<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:32<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:39<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.35s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.35s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:24<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.72s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:21<00:00, 40.54s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'logit_scale', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:37,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<01:00,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:05<00:39,  1.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:35,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:22<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:32<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:39<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.35s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.36s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:24<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.73s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:21<00:00, 40.54s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'logit_scale', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'visual_projection.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'text_projection.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:37,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<01:00,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:05<00:39,  1.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:35,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:22<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:32<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:39<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.36s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.36s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:24<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.72s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:21<00:00, 40.54s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'logit_scale', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'text_projection.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:37,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<01:00,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:05<00:39,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:35,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:22<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:32<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:39<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.34s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.34s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:23<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.70s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:21<00:00, 40.52s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'visual_projection.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'text_projection.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'logit_scale', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:37,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<01:01,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:05<00:39,  1.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:35,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:22<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:32<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:39<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.35s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.35s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:23<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.71s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:21<00:00, 40.53s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n",
      "Global seed set to 42\n",
      "Loading model from /workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\n",
      "Global Step: 2000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'text_projection.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'logit_scale', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:01<01:37,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:02<01:01,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:03<00:48,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:04<00:42,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:05<00:39,  1.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:05<00:36,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:06<00:35,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:07<00:33,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:08<00:32,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:08<00:31,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:09<00:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:10<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:11<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:12<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:13<00:26,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:14<00:25,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:15<00:24,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:15<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:16<00:23,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:17<00:22,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:18<00:21,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:18<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:19<00:20,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:20<00:19,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:21<00:18,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:22<00:17,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:22<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:23<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:24<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:25<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:26<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:27<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:28<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:29<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:30<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:31<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:32<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:33<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:34<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:35<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:36<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:37<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:38<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:39<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:39<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:41<00:00, 41.35s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:41<00:41, 41.35s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (10, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/50 [00:00<00:37,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:01<00:37,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/50 [00:02<00:36,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:03<00:35,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/50 [00:03<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:04<00:34,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▏                         | 7/50 [00:05<00:33,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:06<00:32,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▍                        | 9/50 [00:06<00:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:07<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▍                      | 11/50 [00:08<00:30,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:09<00:29,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  26%|███████▌                     | 13/50 [00:10<00:28,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:10<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  30%|████████▋                    | 15/50 [00:11<00:27,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:12<00:26,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  34%|█████████▊                   | 17/50 [00:13<00:25,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:13<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  38%|███████████                  | 19/50 [00:14<00:24,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:15<00:23,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  42%|████████████▏                | 21/50 [00:16<00:22,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:17<00:21,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  46%|█████████████▎               | 23/50 [00:17<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:18<00:20,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  50%|██████████████▌              | 25/50 [00:19<00:19,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:20<00:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  54%|███████████████▋             | 27/50 [00:20<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:21<00:17,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  58%|████████████████▊            | 29/50 [00:22<00:16,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:23<00:15,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  62%|█████████████████▉           | 31/50 [00:24<00:14,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:24<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  66%|███████████████████▏         | 33/50 [00:25<00:13,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:26<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  70%|████████████████████▎        | 35/50 [00:27<00:11,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:27<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  74%|█████████████████████▍       | 37/50 [00:28<00:10,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:29<00:09,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▌      | 39/50 [00:30<00:08,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:30<00:07,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▊     | 41/50 [00:31<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:32<00:06,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▉    | 43/50 [00:33<00:05,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:34<00:04,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 45/50 [00:34<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:35<00:03,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▎ | 47/50 [00:36<00:02,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:37<00:01,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 49/50 [00:37<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:39<00:00, 39.75s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [01:21<00:00, 40.55s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "outputs/txt2img-samples \n",
      " \n",
      "Enjoy.\n"
     ]
    }
   ],
   "source": [
    "# from https://icons8.com/icons/request-icon\n",
    "prompts = ['a hyena', 'a fox', 'a playstation', 'trash', 'laundry', \n",
    "           'soup', 'a yogurt container', 'fennel', 'a keyboard',\n",
    "           'instagram icon', 'the sims icon', 'galaxy app store',\n",
    "           'amazon app store', 'world of tanks',\n",
    "           'python', 'steam store', 'a phone', 'discord logo',\n",
    "           'twitter logo', 'jupyter notebook',\n",
    "           'a crocodile holding a computer',\n",
    "           'jesus christ crucifix', 'pet family', 'lily flower',\n",
    "           'rose flower', 'elon musk',\n",
    "           'peace sign symbol', 'flexing muscle', 'flexing arm muscle',\n",
    "           'fisherman boat',\n",
    "           'a paintbrush', 'skiing']\n",
    "\n",
    "for prompt in prompts:\n",
    "    !python scripts/stable_txt2img.py \\\n",
    "     --ddim_eta 0.0 \\\n",
    "     --n_samples 10 \\\n",
    "     --n_iter 2 \\\n",
    "     --scale 7.0 \\\n",
    "     --ddim_steps 50 \\\n",
    "     --outdir \"outputs/txt2img-samples\" \\\n",
    "     --ckpt \"/workspace/Dreambooth-Stable-Diffusion/logs/token_class2022-11-09T12-43-09_personicontest/checkpoints/last.ckpt\" \\\n",
    "     --prompt \"black and white icon of {prompt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faccfe3-3042-4471-ba2c-7076812e0f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ddb03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python scripts/stable_txt2img.py \\\n",
    "#  --ddim_eta 0.0 \\\n",
    "#  --n_samples 1 \\\n",
    "#  --n_iter 4 \\\n",
    "#  --scale 7.0 \\\n",
    "#  --ddim_steps 50 \\\n",
    "#  --ckpt \"/workspace/Dreambooth-Stable-Diffusion/trained_models/{file_name}\" \\\n",
    "#  --prompt \"joepenna person as a masterpiece portrait painting by John Singer Sargent in the style of Rembrandt\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
